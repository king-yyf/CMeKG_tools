# -*- coding: utf-8 -*-
"""medical_re.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nddzA4hsk1pr9u1HobaxbAJr2M1QAr-K
"""

import random
import json
import numpy as np
import torch
import torch.nn as nn
# from constant import ProductionConfig as Path
from transformers import BertTokenizer, BertModel, AdamW
from itertools import cycle
import gc
import random
import time
import re

class config:
    # batch_size = 32
    batch_size = 2
    max_seq_len = 256
    # num_p = 23  # 关系数量
    num_p = 48  # 关系数量
    learning_rate = 1e-5
    EPOCH = 5

    # PATH_SCHEMA = "/Users/yangyf/workplace/model_to_save/medical_re/predicate.json"  # 应该是关系定义
    # PATH_TRAIN = '/Users/yangyf/workplace/model_to_save/medical_re/train_data.json'  # 训练数据
    # PATH_BERT = "/Users/yangyf/workplace/model_to_save/medical_re/"
    # PATH_MODEL = "/Users/yangyf/workplace/model_to_save/medical_re/model_re.pkl"
    # PATH_SAVE = '/content/model_re.pkl'
    # tokenizer = BertTokenizer.from_pretrained("/Users/yangyf/workplace/model_to_save/medical_re/" + 'vocab.txt')

    # PATH_SCHEMA = "C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\predicate.json"  # 应该是关系定义
    # PATH_TRAIN = 'C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\train.json'  # 训练数据
    # PATH_BERT = "C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\"
    # PATH_MODEL = "C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\model_re.pkl"
    # PATH_SAVE = 'C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_to_save\\model_re.pkl'
    # tokenizer = BertTokenizer.from_pretrained("C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\" + 'vocab.txt')

    PATH_SCHEMA = "C:\\Users\\Aki\\source\\python\\CMeKG_tools\\duie_data_process\\predicate_unicode.json"  # 应该是关系定义
    PATH_TRAIN = 'C:\\Users\\Aki\\source\\python\\CMeKG_tools\\duie_data_process\\duie_spo_train.json'  # 训练数据
    PATH_BERT = "C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\"
    PATH_MODEL = "C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\model_re.pkl"
    PATH_SAVE = 'C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_to_save\\model_re.pkl'
    tokenizer = BertTokenizer.from_pretrained("C:\\Users\\Aki\\source\\python\\CMeKG_tools\\model_re\\" + 'vocab.txt')

    id2predicate = {}
    predicate2id = {}



class IterableDataset(torch.utils.data.IterableDataset):
    def __init__(self, data, random):
        super(IterableDataset).__init__()
        self.data = data  # 就是一个dict组成的list dict: {'text':'', 'sop_list': [[S1,P1,O1],[S2,P2,O2],[S3,P3,O3]..]}
        self.random = random
        self.tokenizer = config.tokenizer

    def __len__(self):
        return len(self.data)

    def search(self, sequence, pattern):
        n = len(pattern)
        for i in range(len(sequence)):
            if sequence[i:i + n] == pattern:
                return i
        return -1

    def process_data(self):   # 模型训练的时候才会执行这个函数
        idxs = list(range(len(self.data)))  # 生成一系列id
        if self.random:
            np.random.shuffle(idxs)
        batch_size = config.batch_size
        max_seq_len = config.max_seq_len  # 一个text最大长度 = 256
        num_p = config.num_p
        batch_token_ids = np.zeros((batch_size, max_seq_len), dtype=np.int64)  # (m * max_seq_len)  m个text
        batch_mask_ids = np.zeros((batch_size, max_seq_len), dtype=np.int64)  # (m * max_seq_len) 每个text是否参与attention
        batch_segment_ids = np.zeros((batch_size, max_seq_len), dtype=np.int64)  # (m * max_seq_len) 没懂
        batch_subject_ids = np.zeros((batch_size, 2), dtype=np.int64)  # (m * 2) 一个主体的id (start, end)
        batch_subject_labels = np.zeros((batch_size, max_seq_len, 2), dtype=np.int64)  # (m * max_seq_len * 2)
        batch_object_labels = np.zeros((batch_size, max_seq_len, num_p, 2), dtype=np.int64)  # (m * max_seq_len * num_p * 2) 对于一个确定的主体，客体是什么
        batch_i = 0  # batch内的id
        for i in idxs:  # 对于每一个id（样本）开始赋值上面的对象(循环batch_size次就退出，所以不会for完整个len(data))
            text = self.data[i]['text']  # 拿到i的text
            batch_token_ids[batch_i, :] = self.tokenizer.encode(text, max_length=max_seq_len, pad_to_max_length=True,
                                                                add_special_tokens=True)  # 用tokenizer生成了一个 1 * max_seq_len 的向量代表text，塞入batch_token_ids
            batch_mask_ids[batch_i, :len(text) + 2] = 1  # 同样对batch_mask_ids位置，也全部标记为1
            spo_list = self.data[i]['spo_list']  # 拿到i的spo_list, spo_list是这个text里面的关系列表: 'sop_list': [[S1,P1,O1],[S2,P2,O2],[S3,P3,O3]..]
            idx = np.random.randint(0, len(spo_list), size=1)[0]  # idx 是从 0（包括）到len(spo_list)（不包括）之间的一个随机整数 （相当于随机选一个spo）       对于一个text，会不会一些S对应的sop没用来训练？
            s_rand = self.tokenizer.encode(spo_list[idx][0])[1:-1]  # 用tokenizer encode这个spo的S
            s_rand_idx = self.search(list(batch_token_ids[batch_i, :]), s_rand)  # 找到这个S在text原文中的起始位置start
            batch_subject_ids[batch_i, :] = [s_rand_idx, s_rand_idx + len(s_rand) - 1]  # 记录这个S在text里面的start和end，赋值为一个(1,2)的矩阵: [start, end]
            for i in range(len(spo_list)):
                spo = spo_list[i]   # 拿到第i个spo
                s = self.tokenizer.encode(spo[0])[1:-1]   # encode这个spo的s
                p = config.prediction2id[spo[1]]  # 从prediction2id拿到一个p的id（23分类的类别）
                o = self.tokenizer.encode(spo[2])[1:-1]  # encode这个关系的o
                s_idx = self.search(list(batch_token_ids[batch_i]), s)  # 找到这个spo的s在text原文中的位置
                o_idx = self.search(list(batch_token_ids[batch_i]), o)  # 找到这个spo的o在text原文中的位置
                if s_idx != -1 and o_idx != -1:  # 如果s和o都存在(这一步应该是一个保险操作，如果标注正常则不需要加这个判断)
                    batch_subject_labels[batch_i, s_idx, 0] = 1  # s_idx为S的start，=1   对于batch内的某一数据batch_i: [batch_i, s_idx, 0] = 1      [标号=数据序号batch_i, 位置=S起点, 是否是起点=0] = 1  '数据序号batch_i这个数据，它在'S起点'是起点'，这件事是true
                    batch_subject_labels[batch_i, s_idx + len(s) - 1, 1] = 1 # s_idx+ len(s)为S的end，=1            [batch_i, s_idx+len, 1] = 1  [标号=数据序号batch_i, 位置=S终点, 是否是终点=1] = 1  '数据序号batch_i这个数据，它在'S终点'是终点'，这件事是true
                    if s_idx == s_rand_idx:  # 如果随机选的spo的S是这次这个spo的S
                        batch_object_labels[batch_i, o_idx, p, 0] = 1               #  [batch_i, o_idx, 0] = 1      [标号=数据序号batch_i, 位置=O起点, 是否是起点=0] = 1  '数据序号batch_i这个数据，它在'O起点'是起点'，这件事是true
                        batch_object_labels[batch_i, o_idx + len(o) - 1, p, 1] = 1  #  [batch_i, o_idx+len, 1] = 1  [标号=数据序号batch_i, 位置=O终点, 是否是终点=1] = 1  '数据序号batch_i这个数据，它在'O终点'是终点'，这件事是true
            batch_i += 1  # batch_i++往下走
            if batch_i == batch_size or i == idxs[-1]:  # 如果到batch size了
                yield batch_token_ids, batch_mask_ids, batch_segment_ids, batch_subject_labels, batch_subject_ids, batch_object_labels
                batch_token_ids[:, :] = 0
                batch_mask_ids[:, :] = 0
                batch_subject_ids[:, :] = 0
                batch_subject_labels[:, :, :] = 0
                batch_object_labels[:, :, :, :] = 0
                batch_i = 0

    def get_stream(self):
        return cycle(self.process_data())  # 一直循环取数据

    def __iter__(self):
        return self.get_stream()

# 就是
class Model4s(nn.Module):
    def __init__(self, hidden_size=768):
        super(Model4s, self).__init__()
        self.bert = BertModel.from_pretrained(config.PATH_BERT)
        for param in self.bert.parameters():
            param.requires_grad = True
        self.dropout = nn.Dropout(p=0.2)
        self.linear = nn.Linear(in_features=hidden_size, out_features=2, bias=True)  # 全连接输入：768，输出：2
        self.sigmoid = nn.Sigmoid()  # 2分类

    def forward(self, input_ids, input_mask, segment_ids, hidden_size=768):  # 输入：3个东西
        hidden_states = self.bert(input_ids,
                                  attention_mask=input_mask,
                                  token_type_ids=segment_ids)[0]  # (batch_size, sequence_length, hidden_size)
        output = self.sigmoid(self.linear(self.dropout(hidden_states))).pow(2)  # sigmoid：对于每一个字，都要2分类，是起/始的概率值

        return output, hidden_states


class Model4po(nn.Module):
    def __init__(self, num_p=config.num_p, hidden_size=768):
        super(Model4po, self).__init__()
        self.dropout = nn.Dropout(p=0.4)
        self.linear = nn.Linear(in_features=hidden_size, out_features=num_p * 2, bias=True)
        self.sigmoid = nn.Sigmoid()

    def forward(self, hidden_states, batch_subject_ids, input_mask):
        all_s = torch.zeros((hidden_states.shape[0], hidden_states.shape[1], hidden_states.shape[2]),
                            dtype=torch.float32)

        for b in range(hidden_states.shape[0]):
            s_start = batch_subject_ids[b][0]
            s_end = batch_subject_ids[b][1]
            s = hidden_states[b][s_start] + hidden_states[b][s_end]  # S起始特征 + S终止特征
            cue_len = torch.sum(input_mask[b])
            all_s[b, :cue_len, :] = s
        hidden_states += all_s

        output = self.sigmoid(self.linear(self.dropout(hidden_states))).pow(4)  # 预测每一个位置与S的关系

        return output  # (batch_size, max_seq_len, num_p*2)


def load_schema(path):
    with open(path, 'r', encoding='utf-8', errors='replace') as f:
        data = json.load(f)  # 加载所有"关系"
        predicate = list(data.keys())  # 把所有关系提取成list
        prediction2id = {}
        id2predicate = {}
        for i in range(len(predicate)):
            prediction2id[predicate[i]] = i   # dict: 关系 --> id
            id2predicate[i] = predicate[i]    # dict: id --> 关系
    num_p = len(predicate)  # 关系总数量
    config.prediction2id = prediction2id
    config.id2predicate = id2predicate
    config.num_p = num_p


def load_data(path):
    text_spos = []
    with open(path, 'r', encoding='utf-8', errors='replace') as f:
        data = json.load(f)  # 读取训练文件
        for item in data:
            text = item['text']  # 获取text字段
            spo_list = item['spo_list']  # 获取spo_list字段
            text_spos.append({  # 建立 { (text <--> spo_list) } 二元组list
                'text': text,
                'spo_list': spo_list
            })
    return text_spos


def loss_fn(pred, target):
    loss_fct = nn.BCELoss(reduction='none')  # 二分类交叉熵损失
    return loss_fct(pred, target)


def train(train_data_loader, model4s, model4po, optimizer):
    for epoch in range(config.EPOCH):
        begin_time = time.time()
        model4s.train()   # 训练模式
        model4po.train()  # 训练模式
        train_loss = 0.
        for bi, batch in enumerate(train_data_loader):  # batch id 和 这个batch的数据
            if bi >= len(train_data_loader) // config.batch_size:
                break
            batch_token_ids, batch_mask_ids, batch_segment_ids, batch_subject_labels, batch_subject_ids, batch_object_labels = batch  # 取数据
            batch_token_ids = torch.tensor(batch_token_ids, dtype=torch.long)  # batch_token_ids: 就是text中'字'的token化，shape=(m, max_len)
            batch_mask_ids = torch.tensor(batch_mask_ids, dtype=torch.long)    # batch_mask_ids: 上述batch_token_ids中，把token变成1（掩码），shape=(m, max_len)
            batch_segment_ids = torch.tensor(batch_segment_ids, dtype=torch.long)
            batch_subject_labels = torch.tensor(batch_subject_labels, dtype=torch.float)  # batch_subject_labels: 这个对于batch里面每一个数据i，S在text中的起点、终点，shape=(m, max_len, 2)  [样本数量, 位置, 起点？终点]
            batch_object_labels = torch.tensor(batch_object_labels, dtype=torch.float).view(config.batch_size,  # batch_subject_labels: 这个对于batch里面每一个数据i，O在text中的起点、终点，shape=(m, max_len, 2)  [样本数量, 位置, 起点？终点]
                                                                                            config.max_seq_len,
                                                                                            config.num_p * 2)
            batch_subject_ids = torch.tensor(batch_subject_ids, dtype=torch.int) # batch_subject_ids: 记录每个样本的的S的start和end位置:[start, end] 一个batch一共m个样本, 所以shape=(m, 2)

            batch_subject_labels_pred, hidden_states = model4s(batch_token_ids, batch_mask_ids, batch_segment_ids)  # 预测主体：每一个字是起/始的概率
            loss4s = loss_fn(batch_subject_labels_pred, batch_subject_labels.to(torch.float32))  # 交叉熵损失： batch_subject_labels_pred：预测值； batch_subject_labels：真实label
            loss4s = torch.mean(loss4s, dim=2, keepdim=False) * batch_mask_ids
            loss4s = torch.sum(loss4s)
            loss4s = loss4s / torch.sum(batch_mask_ids)

            batch_object_labels_pred = model4po(hidden_states, batch_subject_ids, batch_mask_ids)
            loss4po = loss_fn(batch_object_labels_pred, batch_object_labels.to(torch.float32))
            loss4po = torch.mean(loss4po, dim=2, keepdim=False) * batch_mask_ids
            loss4po = torch.sum(loss4po)
            loss4po = loss4po / torch.sum(batch_mask_ids)

            loss = loss4s + loss4po
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            train_loss += float(loss.item())
            print('batch:', bi, 'loss:', float(loss.item()))

        print('final train_loss:', train_loss / len(train_data_loader) * config.batch_size, 'cost time:',
              time.time() - begin_time)

    del train_data_loader
    gc.collect();

    return {
        "model4s_state_dict": model4s.state_dict(),
        "model4po_state_dict": model4po.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
    }


def extract_spoes(text, model4s, model4po):   # 使用：提取关系spo
    """
    return: a list of many tuple of (s, p, o)
    """
    # 处理text
    with torch.no_grad():
        tokenizer = config.tokenizer
        max_seq_len = config.max_seq_len
        token_ids = torch.tensor(
            tokenizer.encode(text, max_length=max_seq_len, pad_to_max_length=True, add_special_tokens=True)).view(1, -1)
        if len(text) > max_seq_len - 2:
            text = text[:max_seq_len - 2]
        mask_ids = torch.tensor([1] * (len(text) + 2) + [0] * (max_seq_len - len(text) - 2)).view(1, -1)
        segment_ids = torch.tensor([0] * max_seq_len).view(1, -1)
        subject_labels_pred, hidden_states = model4s(token_ids, mask_ids, segment_ids)
        subject_labels_pred = subject_labels_pred.cpu()
        subject_labels_pred[0, len(text) + 2:, :] = 0
        start = np.where(subject_labels_pred[0, :, 0] > 0.4)[0]
        end = np.where(subject_labels_pred[0, :, 1] > 0.4)[0]

        subjects = []
        for i in start:
            j = end[end >= i]
            if len(j) > 0:
                j = j[0]
                subjects.append((i, j))

        if len(subjects) == 0:
            return []
        subject_ids = torch.tensor(subjects).view(1, -1)

        spoes = []
        for s in subjects:
            object_labels_pred = model4po(hidden_states, subject_ids, mask_ids)
            object_labels_pred = object_labels_pred.view((1, max_seq_len, config.num_p, 2)).cpu()
            object_labels_pred[0, len(text) + 2:, :, :] = 0
            start = np.where(object_labels_pred[0, :, :, 0] > 0.4)
            end = np.where(object_labels_pred[0, :, :, 1] > 0.4)

            for _start, predicate1 in zip(*start):
                for _end, predicate2 in zip(*end):
                    if _start <= _end and predicate1 == predicate2:
                        spoes.append((s, predicate1, (_start, _end)))
                        break

    id_str = ['[CLS]']
    i = 1
    index = 0
    while i < token_ids.shape[1]:
        if token_ids[0][i] == 102:
            break

        word = tokenizer.decode(token_ids[0, i:i + 1])
        word = re.sub('#+', '', word)
        if word != '[UNK]':
            id_str.append(word)
            index += len(word)
            i += 1
        else:
            j = i + 1
            while j < token_ids.shape[1]:
                if token_ids[0][j] == 102:
                    break
                word_j = tokenizer.decode(token_ids[0, j:j + 1])
                if word_j != '[UNK]':
                    break
                j += 1
            if token_ids[0][j] == 102 or j == token_ids.shape[1]:
                while i < j - 1:
                    id_str.append('')
                    i += 1
                id_str.append(text[index:])
                i += 1
                break
            else:
                index_end = text[index:].find(word_j)
                word = text[index:index + index_end]
                id_str.append(word)
                index += index_end
                i += 1
    res = []
    for s, p, o in spoes:
        s_start = s[0]
        s_end = s[1]
        sub = ''.join(id_str[s_start:s_end + 1])
        o_start = o[0]
        o_end = o[1]
        obj = ''.join(id_str[o_start:o_end + 1])
        res.append((sub, config.id2predicate[p], obj))

    return res


class SPO(tuple):
    def __init__(self, spo):
        self.spox = (
            tuple(config.tokenizer.tokenize(spo[0])),
            spo[1],
            tuple(config.tokenizer.tokenize(spo[2])),
        )

    def __hash__(self):
        return self.spox.__hash__()

    def __eq__(self, spo):
        return self.spox == spo.spox


def evaluate(data, is_print, model4s, model4po):
    X, Y, Z = 1e-10, 1e-10, 1e-10
    for d in data:
        R = set([SPO(spo) for spo in extract_spoes(d['text'], model4s, model4po)])  # 模型提取出的三元组数目
        T = set([SPO(spo) for spo in d['spo_list']])  # 正确的三元组数目
        if is_print:
            print('text:', d['text'])
            print('R:', R)
            print('T:', T)
        X += len(R & T)  # 模型提取出的三元组数目中正确的个数
        Y += len(R)  # 模型提取出的三元组个数
        Z += len(T)  # 正确的三元组总数
    f1, precision, recall = 2 * X / (Y + Z), X / Y, X / Z

    return f1, precision, recall


def run_train():
    load_schema(config.PATH_SCHEMA)  # 读所有关系,形成列表和id映射
    train_path = config.PATH_TRAIN
    all_data = load_data(train_path)  # 形成[(text, spo)]列表，即[{'text':'SxxPxxOxxx', 'sop_list': [['S0','P0','O0'],['S1','P1','O1'],...]}, {'text':'', 'spo_list':[...]} ...]
    random.shuffle(all_data)

    # 8:2划分训练集、验证集
    idx = int(len(all_data) * 0.8)
    train_data = all_data[:idx]
    valid_data = all_data[idx:]

    # train
    train_data_loader = IterableDataset(train_data, True)
    num_train_data = len(train_data)
    # checkpoint = torch.load(config.PATH_MODEL)

    model4s = Model4s()  # 预测subject(主体)
    # model4s.load_state_dict(checkpoint['model4s_state_dict'])
    # model4s.cuda()

    model4po = Model4po()  # 预测predicate(谓语)和object(客体)
    # model4po.load_state_dict(checkpoint['model4po_state_dict'])
    # model4po.cuda()

    param_optimizer = list(model4s.named_parameters()) + list(model4po.named_parameters())
    no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']
    optimizer_grouped_parameters = [
        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},
        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}
    ]

    lr = config.learning_rate
    optimizer = AdamW(optimizer_grouped_parameters, lr=lr)
    # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

    checkpoint = train(train_data_loader, model4s, model4po, optimizer)  # 开始训练

    del train_data
    gc.collect()
    # save
    model_path = config.PATH_SAVE
    torch.save(checkpoint, model_path)
    print('saved!')

    # valid
    model4s.eval()
    model4po.eval()
    f1, precision, recall = evaluate(valid_data, True, model4s, model4po)
    print('f1: %.5f, precision: %.5f, recall: %.5f' % (f1, precision, recall))


def load_model():
    load_schema(config.PATH_SCHEMA)
    checkpoint = torch.load(config.PATH_MODEL, map_location='cpu')

    model4s = Model4s()
    model4s.load_state_dict(checkpoint['model4s_state_dict'])
    # model4s.cuda()

    model4po = Model4po()
    model4po.load_state_dict(checkpoint['model4po_state_dict'])
    # model4po.cuda()

    return model4s, model4po


def get_triples(content, model4s, model4po):  # 使用模型
    if len(content) == 0:
        return []
    text_list = content.split('。')[:-1]
    res = []
    for text in text_list:
        if len(text) > 128:
            text = text[:128]
        triples = extract_spoes(text, model4s, model4po)
        res.append({
            'text': text,
            'triples': triples
        })
    return res

if __name__ == "__main__":

    run_train()

    # with open(config.PATH_TRAIN, 'r', encoding="utf-8", errors='replace') as f:
    #     data = json.load(f)
    #
    #     f1=open("train.json","w")
    #
    #     json.dump(data,f1,ensure_ascii=False,indent=True)
    #     print("finish")

    # load_schema(config.PATH_SCHEMA)
    # model4s, model4po = load_model()
    #
    # text = "据报道称，新冠肺炎患者经常会发热、咳嗽，少部分患者会胸闷、=乏力，其病因包括: 1.自身免疫系统缺陷\n2.人传人。"
    #
    # res = get_triples(text, model4s, model4po)

    # print(res)

